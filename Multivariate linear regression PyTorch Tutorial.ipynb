{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $y$ dependends linearly on a set of variables $\\mathbf{x}\\in \\mathbb{R}^{m}$ such that $y=b+w_1x_1+w_2x_2+...+w_mx_m$ where $\\mathbf{w}\\in \\mathbb{R}^{m}$ are the weights and $b \\in \\mathbb{R}$ is the bias. Lets say we gather a number of measurements on all features, we can write the observations as a set of linear equations, <br>\n",
    "\n",
    "\\begin{align}\n",
    "    \\begin{bmatrix}\n",
    "       y_{1} \\\\\n",
    "       \\vdots \\\\\n",
    "       y_{i} \\\\\n",
    "       \\vdots \\\\\n",
    "       y_{n}\n",
    "     \\end{bmatrix} =\n",
    "     \\begin{bmatrix}\n",
    "       x_{1,1} & x_{1,2} & \\cdots & x_{1,j} & \\cdots & x_{1,m} \\\\\n",
    "       \\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "       x_{i,1} & x_{i,2} & \\cdots & x_{i,j} & \\cdots & x_{i,m} \\\\\n",
    "       \\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "       x_{n,1} & x_{n,2} & \\cdots & x_{n,j} & \\cdots & x_{n,m}\n",
    "     \\end{bmatrix}\n",
    "     \\begin{bmatrix}\n",
    "       w_{1} \\\\\n",
    "       w_{2} \\\\\n",
    "       \\vdots \\\\\n",
    "       w_{j} \\\\\n",
    "       \\vdots \\\\\n",
    "       w_{m}         \n",
    "     \\end{bmatrix} + b.\n",
    "\\end{align}\n",
    "\n",
    "The loss function $L:\\mathbb{R}^{m+1}\\mapsto\\mathbb{R}$ to minimize w.r.t the weights and bias is, <br>\n",
    "\n",
    "\\begin{align}\n",
    "    L(\\mathbf{w})=\\sum_{i=1}^{n}(\\langle\\ \\mathbf{w},\\mathbf{x}_{i}\\rangle+b-y_{i})^{2}=\\sum_{i=1}^{n}(\\sum_{j=1}^{m}x_{i,j}w_{j}+b-y_{i})^{2}.\n",
    "\\end{align}\n",
    "\n",
    "That is we want, <br>\n",
    "\\begin{align}\n",
    "    \\displaystyle{\\min_{\\mathbf{w}\\in\\mathbb{R}^{m} b\\in\\mathbb{R}} L(\\mathbf{w},b)}.\n",
    "\\end{align}\n",
    "\n",
    "The loss function is minimized at the stationary point given by $\\nabla L(\\mathbf{w},b)=(\\frac{\\partial L(\\mathbf{w},b)}{\\partial w_1},\\frac{\\partial L(\\mathbf{w},b)}{\\partial w_2},...,\\frac{\\partial L(\\mathbf{w},b)}{\\partial w_m},\\frac{\\partial L(\\mathbf{w},b)}{\\partial b})=0$. This stationary point is a global minima since the loss function is convex.\n",
    "\n",
    "The stationary point is found iteratively (gradient descent) by taking a step in the direction of the gradient given by using the first or second order Taylor expansion. That is,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{w}_{t+1}=\\mathbf{w}_{t}-\\alpha \\nabla L(\\mathbf{w}_{t})\\\\\n",
    "    b_{t+1}=b_{t}-\\alpha \\nabla L(b_{t})\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ is the step size or learning rate and, <br>\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial L(\\mathbf{w})}{\\partial w_j}=2\\sum_{i=1}^{n}(\\langle\\ \\mathbf{w},\\mathbf{x}_{i}\\rangle+b-y_{i})x_{i,j} \\\\\n",
    "    \\frac{\\partial L(b)}{\\partial b}=2\\sum_{i=1}^{n}(\\langle\\ \\mathbf{w},\\mathbf{x}_{i}\\rangle+b-y_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0, loss = 13914.150519626783, W_hat = [ 3.46839621 11.60579438 17.92580962], b_hat = 8.459330490087142\n",
      "t = 1000, loss = 11.353998351393074, W_hat = [2.91750236 6.49675418 8.25862308], b_hat = 3.9011841457839256\n",
      "t = 2000, loss = 11.347854771798836, W_hat = [2.91616599 6.49297673 8.25240454], b_hat = 3.8987430621196633\n",
      "t = 3000, loss = 11.347854769083396, W_hat = [2.9161651  6.49297422 8.25240041], b_hat = 3.8987414392232136\n",
      "t = 4000, loss = 11.347854769083371, W_hat = [2.9161651  6.49297422 8.25240041], b_hat = 3.898741438144275\n",
      "t = 5000, loss = 11.347854769083378, W_hat = [2.9161651  6.49297422 8.25240041], b_hat = 3.8987414381436416\n",
      "t = 6000, loss = 11.347854769083378, W_hat = [2.9161651  6.49297422 8.25240041], b_hat = 3.8987414381436416\n",
      "t = 7000, loss = 11.347854769083378, W_hat = [2.9161651  6.49297422 8.25240041], b_hat = 3.8987414381436416\n",
      "t = 8000, loss = 11.347854769083378, W_hat = [2.9161651  6.49297422 8.25240041], b_hat = 3.8987414381436416\n",
      "t = 9000, loss = 11.347854769083378, W_hat = [2.9161651  6.49297422 8.25240041], b_hat = 3.8987414381436416\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "def linear_model(X,w,b):\n",
    "    ''' Y = Xw + b'''\n",
    "    return torch.add(torch.matmul(X,w),b)\n",
    "\n",
    "def loss(Y_hat, Y_obs):\n",
    "    ''' Sum of squared errors on Y_obs after the fit.'''\n",
    "    return ((Y_hat - Y_obs)**2).sum() \n",
    "\n",
    "### True model ###\n",
    "\n",
    "w_true = torch.tensor(np.array([3.,6.,9.]))       # Weights.\n",
    "b_true = torch.tensor([3.])                       # Bias.\n",
    "\n",
    "X_true = torch.tensor(np.linspace((0,1,2),(1,2,3),10))\n",
    "Y_true = linear_model(X_true,w_true,b_true)\n",
    "\n",
    "\n",
    "### Observed data  ###\n",
    "\n",
    "Y_obs = torch.add(Y_true, torch.randn(Y_true.shape))\n",
    "\n",
    "\n",
    "### Model Parameters ###\n",
    "\n",
    "# requires_grad means the tensors store the gradients themselves. False by default.\n",
    "w_hat = torch.randn(w_true.shape, dtype=torch.float64, requires_grad=True) \n",
    "b_hat = torch.randn(1, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "\n",
    "### Hyperparamters ###\n",
    "\n",
    "alpha  = 0.01     # Learning rate.\n",
    "n_iter = 10000    # Time steps (epochs).\n",
    "\n",
    "\n",
    "### Optimizer object ###\n",
    "\n",
    "optimizer = optim.SGD([w_hat, b_hat], lr=alpha) # Holds current state of parameters and updates based on gradients.\n",
    "# help(optimizer)                               \n",
    "# print(whos)\n",
    "\n",
    "\n",
    "### Main optimization loop ### \n",
    "\n",
    "for t in range(n_iter):               \n",
    "    optimizer.zero_grad()                                         # Set the gradients to zero.   \n",
    "    current_loss = loss(linear_model(X_true, w_hat, b_hat),Y_obs) # For tracking the loss.\n",
    "    current_loss.backward()                                       # Compute gradients of loss function (scalar-vector).\n",
    "    optimizer.step()                                              # Update W_hat and b_hat.\n",
    "    if t % 1000 == 0 :\n",
    "        print(f\"t = {t}, loss = {current_loss}, W_hat = {w_hat.detach().numpy()}, b_hat = {b_hat.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SGD in module torch.optim.sgd object:\n",
      "\n",
      "class SGD(torch.optim.optimizer.Optimizer)\n",
      " |  SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
      " |  \n",
      " |  Implements stochastic gradient descent (optionally with momentum).\n",
      " |  \n",
      " |  Nesterov momentum is based on the formula from\n",
      " |  `On the importance of initialization and momentum in deep learning`__.\n",
      " |  \n",
      " |  Args:\n",
      " |      params (iterable): iterable of parameters to optimize or dicts defining\n",
      " |          parameter groups\n",
      " |      lr (float): learning rate\n",
      " |      momentum (float, optional): momentum factor (default: 0)\n",
      " |      weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
      " |      dampening (float, optional): dampening for momentum (default: 0)\n",
      " |      nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
      " |  \n",
      " |  Example:\n",
      " |      >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      " |      >>> optimizer.zero_grad()\n",
      " |      >>> loss_fn(model(input), target).backward()\n",
      " |      >>> optimizer.step()\n",
      " |  \n",
      " |  __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
      " |  \n",
      " |  .. note::\n",
      " |      The implementation of SGD with Momentum/Nesterov subtly differs from\n",
      " |      Sutskever et. al. and implementations in some other frameworks.\n",
      " |  \n",
      " |      Considering the specific case of Momentum, the update can be written as\n",
      " |  \n",
      " |      .. math::\n",
      " |          \\begin{aligned}\n",
      " |              v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n",
      " |              p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n",
      " |          \\end{aligned}\n",
      " |  \n",
      " |      where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the \n",
      " |      parameters, gradient, velocity, and momentum respectively.\n",
      " |  \n",
      " |      This is in contrast to Sutskever et. al. and\n",
      " |      other frameworks which employ an update of the form\n",
      " |  \n",
      " |      .. math::\n",
      " |          \\begin{aligned}\n",
      " |              v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n",
      " |              p_{t+1} & = p_{t} - v_{t+1}.\n",
      " |          \\end{aligned}\n",
      " |  \n",
      " |      The Nesterov version is analogously modified.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SGD\n",
      " |      torch.optim.optimizer.Optimizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  step(self, closure=None)\n",
      " |      Performs a single optimization step.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          closure (callable, optional): A closure that reevaluates the model\n",
      " |              and returns the loss.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add_param_group(self, param_group)\n",
      " |      Add a param group to the :class:`Optimizer` s `param_groups`.\n",
      " |      \n",
      " |      This can be useful when fine tuning a pre-trained network as frozen layers can be made\n",
      " |      trainable and added to the :class:`Optimizer` as training progresses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          param_group (dict): Specifies what Tensors should be optimized along with group\n",
      " |          specific optimization options.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict)\n",
      " |      Loads the optimizer state.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): optimizer state. Should be an object returned\n",
      " |              from a call to :meth:`state_dict`.\n",
      " |  \n",
      " |  state_dict(self)\n",
      " |      Returns the state of the optimizer as a :class:`dict`.\n",
      " |      \n",
      " |      It contains two entries:\n",
      " |      \n",
      " |      * state - a dict holding current optimization state. Its content\n",
      " |          differs between optimizer classes.\n",
      " |      * param_groups - a dict containing all parameter groups\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Clears the gradients of all optimized :class:`torch.Tensor` s.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- Seems to fit average between features but fits the intercept better. ie try different/larger weights and bias.\n",
    "- Analytical solution is very different:\n",
    "\n",
    "Analytical solution:\n",
    "<br>\n",
    "X_analytical = np.column_stack((np.ones(X_true.shape[0]), np.linspace((0,1,2),(1,2,3),10))) <br>\n",
    "w_analytical = np.matmul(np.matmul(np.linalg.inv(np.matmul(X_analytical.transpose(),X_analytical)),  X_analytical.transpose()),Y_true) <br>\n",
    "print(w_analytical.numpy())       <br>\n",
    "print(b_hat.detach().numpy(), w_hat.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.0000, 2.0000],\n",
       "        [0.1111, 1.1111, 2.1111],\n",
       "        [0.2222, 1.2222, 2.2222],\n",
       "        [0.3333, 1.3333, 2.3333],\n",
       "        [0.4444, 1.4444, 2.4444],\n",
       "        [0.5556, 1.5556, 2.5556],\n",
       "        [0.6667, 1.6667, 2.6667],\n",
       "        [0.7778, 1.7778, 2.7778],\n",
       "        [0.8889, 1.8889, 2.8889],\n",
       "        [1.0000, 2.0000, 3.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_true\n",
    "# check svd of (XTX)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "- https://pytorch.org/docs/stable/optim.html\n",
    "- https://pytorch.org/docs/master/generated/torch.tensor.html\n",
    "- https://donaldpinckney.com/books/pytorch/book/ch2-linreg/intro.html\n",
    "- https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944\n",
    "- https://stackoverflow.com/questions/53975717/pytorch-connection-between-loss-backward-and-optimizer-step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
