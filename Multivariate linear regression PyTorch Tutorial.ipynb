{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $y$ dependends linearly on a set of variables $\\mathbf{x}\\in \\mathbb{R}^{m}$ such that $y=b+w_1x_1+w_2x_2+...+w_mx_m$ where $\\mathbf{w}\\in \\mathbb{R}^{m}$ are the weights and $b \\in \\mathbb{R}$ is the bias. Lets say we gather a number of measurements on all features, we can write the observations as a set of linear equations, <br>\n",
    "\n",
    "\\begin{align}\n",
    "    \\begin{bmatrix}\n",
    "       y_{1} \\\\\n",
    "       \\vdots \\\\\n",
    "       y_{i} \\\\\n",
    "       \\vdots \\\\\n",
    "       y_{n}\n",
    "     \\end{bmatrix} =\n",
    "     \\begin{bmatrix}\n",
    "       x_{1,1} & x_{1,2} & \\cdots & x_{1,j} & \\cdots & x_{1,m} \\\\\n",
    "       \\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "       x_{i,1} & x_{i,2} & \\cdots & x_{i,j} & \\cdots & x_{i,m} \\\\\n",
    "       \\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "       x_{n,1} & x_{n,2} & \\cdots & x_{n,j} & \\cdots & x_{n,m}\n",
    "     \\end{bmatrix}\n",
    "     \\begin{bmatrix}\n",
    "       w_{1} \\\\\n",
    "       w_{2} \\\\\n",
    "       \\vdots \\\\\n",
    "       w_{j} \\\\\n",
    "       \\vdots \\\\\n",
    "       w_{n}         \n",
    "     \\end{bmatrix} + b.\n",
    "\\end{align}\n",
    "\n",
    "The loss function $L:\\mathbb{R}^{m+1}\\mapsto\\mathbb{R}$ to minimize w.r.t the weights and bias is, <br>\n",
    "\n",
    "\\begin{align}\n",
    "    L(\\mathbf{w})=\\sum_{i=1}^{n}(\\langle\\ \\mathbf{w},\\mathbf{x}_{i}\\rangle+b-y_{i})^{2}=\\sum_{i=1}^{n}(\\sum_{j=1}^{m}x_{i,j}w_{j}+b-y_{i})^{2}.\n",
    "\\end{align}\n",
    "\n",
    "That is we want, <br>\n",
    "\\begin{align}\n",
    "    \\displaystyle{\\min_{\\mathbf{w}\\in\\mathbb{R}^{m} b\\in\\mathbb{R}} L(\\mathbf{w},b)}.\n",
    "\\end{align}\n",
    "\n",
    "The loss function is minimized at the stationary point given by $\\nabla L(\\mathbf{w},b)=(\\frac{\\partial L(\\mathbf{w},b)}{\\partial w_1},\\frac{\\partial L(\\mathbf{w},b)}{\\partial w_2},...,\\frac{\\partial L(\\mathbf{w},b)}{\\partial w_m},\\frac{\\partial L(\\mathbf{w},b)}{\\partial b})=0$. This stationary point is a global minima since the loss function is convex.\n",
    "\n",
    "The stationary point is found iteratively (gradient descent) by taking a step in the direction of the gradient given by using the first or second order Taylor expansion. That is,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{w}_{t+1}=\\mathbf{w}_{t}-\\alpha \\nabla L(\\mathbf{w}_{t})\\\\\n",
    "    b_{t+1}=b_{t}-\\alpha \\nabla L(b_{t})\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ is the step size or learning rate and, <br>\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial L(\\mathbf{w})}{\\partial w_j}=2\\sum_{i=1}^{n}(\\langle\\ \\mathbf{w},\\mathbf{x}_{i}\\rangle+b-y_{i})x_{i,j} \\\\\n",
    "    \\frac{\\partial L(b)}{\\partial b}=2\\sum_{i=1}^{n}(\\langle\\ \\mathbf{w},\\mathbf{x}_{i}\\rangle+b-y_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0, loss = 13914.150519626783, W_hat = [ 3.46839621 11.60579438 17.92580962], b_hat = 8.459330490087142\n",
      "t = 1000, loss = 11.353998351393074, W_hat = [2.91750236 6.49675418 8.25862308], b_hat = 3.9011841457839256\n",
      "t = 2000, loss = 11.347854771798836, W_hat = [2.91616599 6.49297673 8.25240454], b_hat = 3.8987430621196633\n",
      "t = 3000, loss = 11.347854769083396, W_hat = [2.9161651  6.49297422 8.25240041], b_hat = 3.8987414392232136\n",
      "t = 4000, loss = 11.347854769083371, W_hat = [2.9161651  6.49297422 8.25240041], b_hat = 3.898741438144275\n",
      "t = 5000, loss = 11.347854769083378, W_hat = [2.9161651  6.49297422 8.25240041], b_hat = 3.8987414381436416\n",
      "t = 6000, loss = 11.347854769083378, W_hat = [2.9161651  6.49297422 8.25240041], b_hat = 3.8987414381436416\n",
      "t = 7000, loss = 11.347854769083378, W_hat = [2.9161651  6.49297422 8.25240041], b_hat = 3.8987414381436416\n",
      "t = 8000, loss = 11.347854769083378, W_hat = [2.9161651  6.49297422 8.25240041], b_hat = 3.8987414381436416\n",
      "t = 9000, loss = 11.347854769083378, W_hat = [2.9161651  6.49297422 8.25240041], b_hat = 3.8987414381436416\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "def linear_model(X,w,b):\n",
    "    ''' Y = Xw + b'''\n",
    "    return torch.add(torch.matmul(X,w),b)\n",
    "\n",
    "def loss(Y_hat, Y_obs):\n",
    "    ''' Sum of squared errors on Y_obs after the fit.'''\n",
    "    return ((Y_hat - Y_obs)**2).sum() \n",
    "\n",
    "### True model ###\n",
    "\n",
    "w_true = torch.tensor(np.array([3.,6.,9.]))       # Weights.\n",
    "b_true = torch.tensor([3.])                       # Bias.\n",
    "\n",
    "X_true = torch.tensor(np.linspace((0,1,2),(1,2,3),10))\n",
    "Y_true = linear_model(X_true,w_true,b_true)\n",
    "\n",
    "\n",
    "### Observed data  ###\n",
    "\n",
    "Y_obs = torch.add(Y_true, torch.randn(Y_true.shape))\n",
    "\n",
    "\n",
    "### Model Parameters ###\n",
    "\n",
    "# requires_grad means the tensors store the gradients themselves. False by default.\n",
    "w_hat = torch.randn(w_true.shape, dtype=torch.float64, requires_grad=True) \n",
    "b_hat = torch.randn(1, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "\n",
    "### Hyperparamters ###\n",
    "\n",
    "alpha  = 0.01     # Learning rate.\n",
    "n_iter = 10000    # Time steps (epochs).\n",
    "\n",
    "\n",
    "### Optimizer object ###\n",
    "\n",
    "optimizer = optim.SGD([w_hat, b_hat], lr=alpha) # Holds current state of parameters and updates based on gradients.\n",
    "# help(optimizer)                               \n",
    "# print(whos)\n",
    "\n",
    "\n",
    "### Main optimization loop ### \n",
    "\n",
    "for t in range(n_iter):               \n",
    "    optimizer.zero_grad()                                         # Set the gradients to zero.   \n",
    "    current_loss = loss(linear_model(X_true, w_hat, b_hat),Y_obs) # For tracking the loss.\n",
    "    current_loss.backward()                                       # Compute gradients of loss function (scalar-vector).\n",
    "    optimizer.step()                                              # Update W_hat and b_hat.\n",
    "    if t % 1000 == 0 :\n",
    "        print(f\"t = {t}, loss = {current_loss}, W_hat = {w_hat.detach().numpy()}, b_hat = {b_hat.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- Seems to fit average between features but fits the intercept better. ie try different/larger weights and bias.\n",
    "- Analytical solution is very different:\n",
    "\n",
    "Analytical solution:\n",
    "<br>\n",
    "X_analytical = np.column_stack((np.ones(X_true.shape[0]), np.linspace((0,1,2),(1,2,3),10))) <br>\n",
    "w_analytical = np.matmul(np.matmul(np.linalg.inv(np.matmul(X_analytical.transpose(),X_analytical)),  X_analytical.transpose()),Y_true) <br>\n",
    "print(w_analytical.numpy())       <br>\n",
    "print(b_hat.detach().numpy(), w_hat.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "- https://pytorch.org/docs/stable/optim.html\n",
    "- https://pytorch.org/docs/master/generated/torch.tensor.html\n",
    "- https://donaldpinckney.com/books/pytorch/book/ch2-linreg/intro.html\n",
    "- https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944\n",
    "- https://stackoverflow.com/questions/53975717/pytorch-connection-between-loss-backward-and-optimizer-step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
