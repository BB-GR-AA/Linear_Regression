{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the training procces of a Linear model using TensorBoard\n",
    "\n",
    "Same data set as the one used in [this notebook](Multivariate_linear_regression_PyTorch_Tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0, loss = 13914.150519626783, W_hat = [-0.1901322   1.32441227  1.02157383], b_hat = 1.8364768028217986\n",
      "t = 1000, loss = 11.428052210609334, W_hat = [2.68091367 6.40062664 8.30295668], b_hat = 4.0416452902123625\n",
      "t = 2000, loss = 11.34987058184224, W_hat = [2.87886783 6.47833323 8.26041571], b_hat = 3.921397727032768\n",
      "t = 3000, loss = 11.347905437795522, W_hat = [2.91025191 6.490653   8.25367117], b_hat = 3.9023334156780782\n",
      "t = 4000, loss = 11.347856042673088, W_hat = [2.91522761 6.49260621 8.25260188], b_hat = 3.899310918108738\n",
      "t = 5000, loss = 11.347854801095838, W_hat = [2.91601647 6.49291587 8.25243235], b_hat = 3.8988317247412696\n",
      "t = 6000, loss = 11.347854769888043, W_hat = [2.91614154 6.49296497 8.25240547], b_hat = 3.8987557523779603\n",
      "t = 7000, loss = 11.3478547691036, W_hat = [2.91616137 6.49297275 8.25240121], b_hat = 3.898743707553511\n",
      "t = 8000, loss = 11.347854769083916, W_hat = [2.91616451 6.49297398 8.25240053], b_hat = 3.8987417979407306\n",
      "t = 9000, loss = 11.347854769083408, W_hat = [2.91616501 6.49297418 8.25240043], b_hat = 3.8987414951865507\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter #Class to log data.\n",
    "from utils import linear_model, SSE\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "### Data ###\n",
    "\n",
    "w_true = torch.tensor(np.array([3.,6.,9.]))       \n",
    "b_true = torch.tensor([3.])                       \n",
    "X_true = torch.tensor(np.linspace((0,1,2),(1,2,3),10))\n",
    "Y_true = linear_model(X_true,w_true,b_true)\n",
    "\n",
    "Y_obs = torch.add(Y_true, torch.randn(Y_true.shape))\n",
    "\n",
    "\n",
    "### Model Parameters ###\n",
    "\n",
    "w_hat = torch.randn(w_true.shape, dtype=torch.float64, requires_grad=True) \n",
    "b_hat = torch.randn(1, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "\n",
    "### Hyperparamters ### \n",
    "\n",
    "alpha  = 0.001      # Learning rate.\n",
    "n_iter = 10000         # Time steps (epochs).\n",
    "optimizer = optim.SGD([w_hat, b_hat], lr=alpha) \n",
    "\n",
    "\n",
    "### Main Optimization Loop ###\n",
    "\n",
    "for t in range(n_iter):               \n",
    "    optimizer.zero_grad()                                         # Set the gradients to zero.   \n",
    "    current_loss = SSE(linear_model(X_true, w_hat, b_hat),Y_obs)  # For tracking the loss.\n",
    "    current_loss.backward()                                       # Compute gradients of loss function (scalar-vector).\n",
    "    optimizer.step()                                              # Update W_hat and b_hat.\n",
    "    if t % 1000 == 0 :\n",
    "        print(f\"t = {t}, loss = {current_loss}, W_hat = {w_hat.detach().numpy()}, b_hat = {b_hat.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview and installation\n",
    "\n",
    "TensorBoard is part of TensorFlow and offers a framework for visualizing and monitoring the training process. It is useful for visualizing the computational graph of nn models, scalars (loss, classification accuracy), and distributions (weights of the model). \n",
    "\n",
    "**Check installation**\n",
    "```python\n",
    ">> conda list | grep tensor\n",
    ">> pip list | grep tensor\n",
    "```\n",
    "\n",
    "**Install**\n",
    "```python\n",
    "# Using anaconda package manager\n",
    ">> conda install tensorflow\n",
    "\n",
    "# Using pip\n",
    ">> pip install --upgrade pip\n",
    ">> pip install tensorflow\n",
    "```\n",
    "\n",
    "**Check instalation**\n",
    "```python\n",
    ">> conda list | grep tensor\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.tensorboard import SummaryWriter #Class to log data.\n",
    "\n",
    "### Model Parameters ###\n",
    "\n",
    "w_hat = torch.randn(w_true.shape, dtype=torch.float64, requires_grad=True) \n",
    "b_hat = torch.randn(1, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "\n",
    "### Hyperparamters ### \n",
    "\n",
    "alpha  = 0.000001      # Learning rate.\n",
    "n_iter = 20000         # Time steps (epochs).\n",
    "optimizer = optim.SGD\n",
    "\n",
    "\n",
    "### TensorBoard Writer Setup ###\n",
    "\n",
    "# We tell Pytorch where to save a log of the trained weights and loss values.\n",
    "log_name = f\"{alpha}, {optimizer.__name__}\"\n",
    "writer = SummaryWriter(log_dir=f\"runs/{log_name}\")\n",
    "print(\"To see tensorboard, run: tensorboard --logdir=runs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whos\n",
    "writer = SummaryWriter(log_dir=f\"runs/{log_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gonzalo\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:40: UserWarning: h5py is running against HDF5 1.10.5 when it was built against 1.10.4, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "### Main Optimization Loop ###\n",
    "\n",
    "for t in range(n_iter):               \n",
    "    optimizer.zero_grad()                                         # Set the gradients to zero.   \n",
    "    current_loss = SSE(linear_model(X_true, w_hat, b_hat),Y_obs)  # For tracking the loss.\n",
    "    current_loss.backward()                                       # Compute gradients of loss function (scalar-vector).\n",
    "    optimizer.step()                                              # Update W_hat and b_hat.\n",
    "    if t % 1000 == 0 :\n",
    "        print(f\"t = {t}, loss = {current_loss}, W_hat = {w_hat.detach().numpy()}, b_hat = {b_hat.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable        Type     Data/Info\n",
      "----------------------------------\n",
      "SummaryWriter   type     <class 'torch.utils.tenso<...>rd.writer.SummaryWriter'>\n",
      "alpha           float    1e-06\n",
      "n_iter          int      20000\n"
     ]
    }
   ],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining summaries\n",
    "\n",
    "??? are objects understood by TensorBoard that we want to display. There are various objects depending on what we want to visualize: <br>\n",
    "- scalars (gradients, loss)\n",
    "- vectors/histogram (model parameters - weights and biases) <br>\n",
    "\n",
    "Furthermore, you can use tf.name_scope to group scalars on the board. That is, scalars having the same name scope will be displayed on the same row. Here you define three different summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting TensorBoard\n",
    "\n",
    "**From the terminal**\n",
    "```python\n",
    "# If in the firectory where ? is saved\n",
    ">> tensorboard --logdir\n",
    ">> tensorboard --logdir=runs\n",
    "\n",
    "# Otherwise\n",
    ">> tensorboard --logdir /path/to/directory/\n",
    "```\n",
    "\n",
    "**From the notebook**\n",
    "```python\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /path/to/directory/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "- https://www.tensorflow.org/install\n",
    "- https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/\n",
    "- https://donaldpinckney.com/books/pytorch/book/ch2-linreg/2017-12-27-optimization.html\n",
    "- https://pytorch.org/docs/stable/tensorboard.html#:~:text=The%20SummaryWriter%20class%20provides%20a,summaries%20and%20events%20to%20it.&text=Use%20hierarchical%20folder%20structure%20to,experiment%20to%20compare%20across%20them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
